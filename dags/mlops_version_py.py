# -*- coding: utf-8 -*-
"""MLOps_version.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Jg5f0chJSFV9Esw79v_2rJaS74j_PAh1
"""

from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime

# Função para calcular limites de outliers
def lim_outliers(coluna):
    import pandas as pd
    q1 = coluna.quantile(0.25)
    q3 = coluna.quantile(0.75)
    iqr = q3 - q1
    lim_inf = q1 - (1.5 * iqr)
    lim_sup = q3 + (1.5 * iqr)
    return pd.Series([lim_inf, lim_sup], index=["lim_inf", "lim_sup"])

# Função para avaliação de modelo com validação cruzada
def evaluate_model_with_cv(model, X, y, cv=5):
    from sklearn.model_selection import cross_val_score
    scores = cross_val_score(model, X, y, cv=cv, scoring='roc_auc', n_jobs=-1)
    return scores.mean(), scores.std()

# Função para definir a data de filtragem do dataset

def data_filtro(data):
    import pandas as pd
    df = pd.read_csv("dags/predictive_maintenance.csv",parse_dates=["date_data"])
    df["date_data"] = pd.to_datetime(df["date_data"]).dt.normalize()
    df = df[df["date_data"]<=data].drop(columns="date_data")
    print (f"Running date = {data}")
    return df


# Task: Preparação dos dados de treino
def prepare_data_train(**context):
    import pandas as pd
    import numpy as np
    from sklearn.preprocessing import StandardScaler, LabelEncoder

    data = pd.Timestamp(context["logical_date"]).tz_localize(None).normalize()
    df = data_filtro(data)
    print("Data extracted!")

    df = df.drop(columns=["UDI"])
    df_grouped = df.groupby("Type").agg(
        lim_inf_air_temp=("Air temperature [K]", lambda x: lim_outliers(x)["lim_inf"]),
        lim_sup_air_temp=("Air temperature [K]", lambda x: lim_outliers(x)["lim_sup"]),
        lim_inf_process_temp=("Process temperature [K]", lambda x: lim_outliers(x)["lim_inf"]),
        lim_sup_process_temp=("Process temperature [K]", lambda x: lim_outliers(x)["lim_sup"]),
        lim_inf_rotational_speed=("Rotational speed [rpm]", lambda x: lim_outliers(x)["lim_inf"]),
        lim_sup_rotational_speed=("Rotational speed [rpm]", lambda x: lim_outliers(x)["lim_sup"]),
        lim_inf_torque=("Torque [Nm]", lambda x: lim_outliers(x)["lim_inf"]),
        lim_sup_torque=("Torque [Nm]", lambda x: lim_outliers(x)["lim_sup"]),
        lim_inf_tw=("Tool wear [min]", lambda x: lim_outliers(x)["lim_inf"]),
        lim_sup_tw=("Tool wear [min]", lambda x: lim_outliers(x)["lim_sup"])
    ).reset_index()
    df_outlier = df.drop(columns="Target").merge(df_grouped, on=["Type"])

    i = 8
    for col in ["Air temperature [K]", "Process temperature [K]", "Rotational speed [rpm]", "Torque [Nm]", "Tool wear [min]"]:
        df_outlier[col] = np.where(
            (df_outlier[col] < df_outlier.iloc[:, i]) | (df_outlier[col] > df_outlier.iloc[:, i + 1]),
            True,
            False
        )
        i += 2
    df_outlier = df_outlier.drop(columns=[
        "lim_inf_air_temp", "lim_sup_air_temp", "lim_inf_process_temp", "lim_sup_process_temp",
        "lim_inf_rotational_speed", "lim_sup_rotational_speed", "lim_inf_torque", "lim_sup_torque",
        "lim_inf_tw", "lim_sup_tw"
    ])
    df_outlier["remove"] = np.where(
        ((df_outlier["Rotational speed [rpm]"] == True) | (df_outlier["Torque [Nm]"] == True)) &
        (df_outlier["Failure Type"] != "Power Failure"),
        True,
        False
    )
    df_removed_outliers = df_outlier[df_outlier["remove"] == False]
    ids_mantidos = df_removed_outliers["Product ID"]
    df_no_outlier = df[df["Product ID"].isin(ids_mantidos)]

    print("Outliers removed")

    df_padronizado = df_no_outlier.copy()
    df_padronizado["UDI"] = 1
    dummies_type = pd.get_dummies(df_padronizado["Type"], prefix="type", drop_first=True).astype(int)
    df_padronizado = pd.concat([df_padronizado, dummies_type], axis=1)
    df_padronizado = df_padronizado.drop(columns=["Product ID", "Type", "UDI"])
    col_padronizar = df_padronizado.drop(columns=["Failure Type", "Target"]).columns
    scaler = StandardScaler()
    df_padronizado[col_padronizar] = scaler.fit_transform(df_padronizado[col_padronizar])
    encoder = LabelEncoder()
    df_padronizado["Failure Type"] = encoder.fit_transform(df_padronizado["Failure Type"])

    print("Dataset padronized")

    df_padronizado.to_csv(f"{context['dag_run'].run_id}_df_padronizado.csv", index=False)

# Task: Divisão dos dados para treino e teste
def train_test_bin_model(**context):
    import numpy as np
    import pandas as pd
    from sklearn.model_selection import train_test_split

    print("Spliting train test datasets")
    df_padronizado = pd.read_csv(f"{context['dag_run'].run_id}_df_padronizado.csv")
    X = df_padronizado.drop(columns=["Failure Type", "Target"])
    Y = df_padronizado["Target"]

    x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)
    np.save(f"{context['dag_run'].run_id}_x_train.npy", x_train)
    np.save(f"{context['dag_run'].run_id}_x_test.npy", x_test)
    np.save(f"{context['dag_run'].run_id}_y_train.npy", y_train)
    np.save(f"{context['dag_run'].run_id}_y_test.npy", y_test)

    print("x_train, x_test, y_train, y_test ready")

# Task: Treinamento do melhor modelo binário
def train_best_model_bin(**context):
    import numpy as np
    import pickle
    from sklearn.linear_model import LogisticRegression
    from sklearn.neighbors import KNeighborsClassifier
    from sklearn.ensemble import RandomForestClassifier

    print("Searching best binary model")
    x_train = np.load(f"{context['dag_run'].run_id}_x_train.npy", allow_pickle=True)
    y_train = np.load(f"{context['dag_run'].run_id}_y_train.npy", allow_pickle=True)

    models_bin = {
        "Logistic Regression - Pesos automáticos": LogisticRegression(random_state=42, max_iter=4000, class_weight='balanced'),
        "Logistic Regression - Pesos manuais": LogisticRegression(class_weight={0:0.507901, 1:2.955361}, max_iter=4000),
        "KNN": KNeighborsClassifier(n_neighbors=3),
        "RandomForestClassifier": RandomForestClassifier(class_weight={0:0.697901, 1:11.955361})
    }

    model_scores = {}
    for model_name, model in models_bin.items():
        mean_score, std_score = evaluate_model_with_cv(model, x_train, y_train)
        model_scores[model_name] = (mean_score, std_score)

    best_model_name = max(model_scores, key=lambda x: model_scores[x][0])
    best_model_01 = models_bin[best_model_name]

    print(f"Best model found: {best_model_01}")

    best_model_01.fit(x_train, y_train)

    with open(f"{context['dag_run'].run_id}_model_binary.pkl", "wb") as f:
        pickle.dump(best_model_01, f)

    print("Model trained and saved")

# Task: Predição nos dados de teste
def predict_on_test_data(**context):
    import numpy as np
    import pickle

    with open(f"{context['dag_run'].run_id}_model_binary.pkl", "rb") as f:
        binary_model = pickle.load(f)
    x_test = np.load(f"{context['dag_run'].run_id}_x_test.npy", allow_pickle=True)
    y_pred = binary_model.predict(x_test)
    np.save(f"{context['dag_run'].run_id}_y_pred.npy", y_pred)
    print("Predicted classes in test dataset")

# Task: Avaliação das métricas
def get_metrics(**context):
    import numpy as np
    from sklearn.metrics import classification_report

    y_test = np.load(f"{context['dag_run'].run_id}_y_test.npy", allow_pickle=True)
    y_pred = np.load(f"{context['dag_run'].run_id}_y_pred.npy", allow_pickle=True)
    print(classification_report(y_test, y_pred))

# Configuração da DAG
with DAG(
    dag_id="predictive_maintenance_pipeline",
    start_date=datetime(2023, 1, 1),
    schedule_interval="@daily",
    catchup=False
) as dag:

    t1 = PythonOperator(
        task_id="prepare_data_train",
        python_callable=prepare_data_train,
        provide_context=True,
    )

    t2 = PythonOperator(
        task_id="train_test_bin_model",
        python_callable=train_test_bin_model,
        provide_context=True,
    )

    t3 = PythonOperator(
        task_id="train_best_model_bin",
        python_callable=train_best_model_bin,
        provide_context=True,
    )

    t4 = PythonOperator(
            task_id="predict_on_test_data",
        python_callable=predict_on_test_data,
        provide_context=True,
    )

    t5 = PythonOperator(
        task_id="get_metrics",
        python_callable=get_metrics,
        provide_context=True,
    )

    # Definindo a sequência de execução
    t1 >> t2 >> t3 >> t4 >> t5